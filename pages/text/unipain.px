<page title="Pragmatic Unicode">
<history>
<what when="20120310T114500">Created.</what>
</history>

<p>This is a presentation I gave at PyCon 2012.
You can read the slides and text on this page, or open the <a href="text/unipain/unipain.html">actual presentation</a> in your browser,
or watch the video:</p>

<p align="center">
<iframe width="500" height="369" src="http://www.youtube.com/embed/sgHbC6udIqc" frameborder="0" allowfullscreen="allowfullscreen"/>
</p>

<p>Also, clicking the slide images will jump into the full presentation at that point.
The Symbola font is included, but will have to be downloaded before some of the
special symbols will appear.</p>

<figurep href="text/unipain/unipain.html#1"><img src="text/unipain_000.png" alt="Pragmatic Unicode~ or ~How Do I Stop the Pain?"/></figurep><p>Hi, I'm Ned Batchelder.  I've been writing in Python for over ten years,
    which means at least a half-dozen times, I've made the same Unicode
    mistakes that everyone else has.</p><figurep href="text/unipain/unipain.html#2"><img src="text/unipain_001.png" alt="The past"/></figurep><p>If you're like most Python programmers, you've done it too: you've built
    a nice application, and everything seemed to be going fine.  Then one day
    an accented character appeared out of nowhere, and your program started
    belching UnicodeErrors.</p><p>You kind of knew what to do with those, so you added an encode or a
    decode where the error was raised, but the UnicodeError happened somewhere
    else.  You went to the new place, and added a decode, maybe an encode.
    After playing whack-a-mole like this for a while, the problem seemed to be
    fixed.</p><p>Then a few days later, another accent appeared in another place, and you
    had to play a little bit more whack-a-mole until the problem finally
    stopped.</p><figurep href="text/unipain/unipain.html#3"><img src="text/unipain_002.png" alt="You"/></figurep><p>So now you have a program that works, but you're annoyed and
    uncomfortable, it took too long, you know it isn't "right," and you hate
    yourself.  And the main thing you know about Unicode is that you don't like
    Unicode.</p><p>You don't want to know about weirdo character sets, you just want 
    to be able to write a program that doesn't make you feel bad.</p><figurep href="text/unipain/unipain.html#4"><img src="text/unipain_003.png" alt="This talk"/></figurep><p>You don't have to play whack-a-mole.  Unicode isn't simple, but it isn't
    difficult either. With knowledge and discipline, you can deal with Unicode
    easily and with grace.</p><p>I'll teach you five Facts of Life, and give you three Pro Tips that will
    solve your Unicode problems. We're going to cover the basics of Unicode,
    and how both Python 2 and Python 3 work.  They are different, but the
    strategies you'll use are basically the same.</p><h1>The World &amp; Unicode</h1><figurep href="text/unipain/unipain.html#5"><img src="text/unipain_004.png" alt="The World &amp; Unicode"/></figurep><p>We'll start with the basics of Unicode.</p><figurep href="text/unipain/unipain.html#6"><img src="text/unipain_005.png" alt="Bytes"/></figurep><p>The first Fact of Life: everything in a computer is bytes.  Files on
    disk are a series of bytes, and network connections only transmit bytes.
    Almost without exception, all the data going into or out of any program you
    write, is bytes.</p><p>The problem with bytes is that by themselves they are meaningless, we
    need conventions to give them meaning.</p><figurep href="text/unipain/unipain.html#7"><img src="text/unipain_006.png" alt="ASCII"/></figurep><p>To represent text, we've been using the ASCII code for nearly 50 years.
    Every byte is assigned one of 95 symbols. When I send you a byte 65, you
    know that I mean an upper-case A, but only because we've agreed beforehand
    on what each byte represents.</p><figurep href="text/unipain/unipain.html#8"><img src="text/unipain_007.png" alt="ISO 8859-1"/></figurep><p>ISO Latin 1, or 8859-1, is ASCII extended with 96 more symbols.</p><figurep href="text/unipain/unipain.html#9"><img src="text/unipain_008.png" alt="Windows-1252"/></figurep><p>Windows added 27 more symbols to produce CP1252.  This is pretty much
    the best you can do to represent text as single bytes, because there's not
    much room left to add more symbols.</p><figurep href="text/unipain/unipain.html#10"><img src="text/unipain_009.png" alt="Tower of Babel"/></figurep><p>With character sets like these, we can represent at most 256 characters.
    But Fact of Life #2 is that there are way more than 256 symbols in the
    world's text.  A single byte simply can't represent text world-wide.
    During your darkest whack-a-mole moments, you may have wished that everyone
    spoke English, but it simply isn't so. People need lots of symbols to
    communicate.</p><p>Fact of Life #1 and Fact of Life #2 together create a fundamental
    conflict between the structure of our computing devices, and the needs of
    the world's people.</p><figurep href="text/unipain/unipain.html#11"><img src="text/unipain_010.png" alt="Character codes"/></figurep><p>There have been a number of attempts to resolve this conflict.
    Single-byte character codes like ASCII map bytes to characters.
    Each one pretends that Fact of Life #2 doesn't exist.</p><p>There are many single-byte codes, and they don't solve the problem.
    Each is only good for representing one small slice of human language.  They
    can't solve the global text problem.</p><figurep href="text/unipain/unipain.html#12"><img src="text/unipain_011.png" alt="Character codes"/></figurep><p>People tried creating double-byte character sets, but they were still
    fragmented, serving different subsets of people.   There were multiple
    standards in place, and they still weren't large enough to deal
    with all the symbols needed.</p><figurep href="text/unipain/unipain.html#13"><img src="text/unipain_012.png" alt="Unicode"/></figurep><p>Unicode was designed to deal decisively with the issues with
    older character codes.  Unicode assigns integers, known as code points, to
    characters.  It has room for 1.1 million code points, and only 110,000 are
    already assigned, so there's plenty of room for future growth.</p><p>Unicode's goal is to have everything.  It starts with ASCII, and
    includes thousands of symbols, including the famous Snowman, covers all the
    writing systems of the world, and is constantly being expanded.  For
    example, the latest update gave us the symbol PILE OF POO.</p><figurep href="text/unipain/unipain.html#14"><img src="text/unipain_013.png" alt="Sample Unicode"/></figurep><p>Here is a string of six exotic Unicode characters.  Unicode code points
    are written as 4-, 5-, or 6-digits of hex with a U+ prefix.  Every
    character has an unambiguous full name which is always in uppercase ASCII.
    </p><p>This string is designed to look like the word "Python", but doesn't use
    any ASCII characters at all.</p><figurep href="text/unipain/unipain.html#15"><img src="text/unipain_014.png" alt="Encodings"/></figurep><p>So Unicode makes room for all of the characters we could ever need,
    but we still have Fact of Life #1 to deal with: computers need bytes.
    We need a way to represent Unicode code points as bytes in order to
    store or transmit them.</p><p>The Unicode standard defines a number of ways to represent code points
    as bytes.  These are called encodings.</p><figurep href="text/unipain/unipain.html#16"><img src="text/unipain_015.png" alt="UTF-8"/></figurep><p>UTF-8 is easily the most popular encoding for storage and transmission
    of Unicode.  It uses a variable number of bytes for each code point. The
    higher the code point value, the more bytes it needs in UTF-8. ASCII
    characters are one byte each, using the same values as ASCII, so ASCII is a
    subset of UTF-8.</p><p>Here we show our exotic string as UTF-8.  The ASCII characters H and i are
    single bytes, and other characters use two or three bytes depending on their
    code point value. Some Unicode code points require four bytes, but we aren't 
    using any of those here.</p><h1>Python 2</h1><figurep href="text/unipain/unipain.html#17"><img src="text/unipain_016.png" alt="Python 2"/></figurep><p>OK, enough theory, let's talk about Python 2. In the slides, Python 2
    samples have a big 2 in the upper-right corner, Python 3 samples will have
    a big 3.</p><figurep href="text/unipain/unipain.html#18"><img src="text/unipain_017.png" alt="Str vs Unicode"/></figurep><p>In Python 2, there are two different string data types.  A plain-old
    string literal gives you a "str" object, which stores bytes.  If you use a
    "u" prefix, you get a "unicode" object, which stores code points.  In a
    unicode string literal, you can use backslash-u to insert any Unicode code
    point.</p><p>Notice that the word "string" is problematic.  Both "str" and "unicode"
    are kinds of strings, and it's tempting to call either or both of them
    "string," but it's better to use more specific terms to keep things
    straight.</p><figurep href="text/unipain/unipain.html#19"><img src="text/unipain_018.png" alt=".encode() and .decode()"/></figurep><p>Bytes strings and unicode strings each have a method to convert it to
    the other type of string.  Unicode
    strings have a .encode() method that produces bytes, and byte strings have
    a .decode() method that produces unicode. Each takes an argument, which
    is the name of the encoding to use for the operation.</p><p>We can define a Unicode string named my_unicode, and see that it has
    9 characters.  We can encode it to UTF-8 to create the my_utf8 byte string,
    which has 19 bytes.  As you'd expect, reversing the operation by decoding 
    the UTF-8 string produces the original Unicode string.</p><figurep href="text/unipain/unipain.html#20"><img src="text/unipain_019.png" alt="Encoding errors"/></figurep><p>Unfortunately, encoding and decoding can produce errors if the data
    isn't appropriate for the specified encoding.  Here we try to encode our
    exotic Unicode string to ASCII.  It fails because ASCII can only represent
    charaters in the range 0 to 127, and our Unicode string has code points
    outside that range.</p><p>The UnicodeEncodeError that's raised indicates the encoding being used,
    in the form of the "codec" (short for coder/decoder), and the actual
    position of the character that caused the problem.</p><figurep href="text/unipain/unipain.html#21"><img src="text/unipain_020.png" alt="Decoding errors"/></figurep><p>Decoding can also produce errors.  Here we try to decode our UTF-8
    string as ASCII and get a UnicodeDecodeError because again, ASCII can only
    accepts values up to 127, and our UTF-8 string has bytes outside that
    range.</p><p>Even UTF-8 can't decode any sequence of bytes.  Next we try to decode
    some random junk, and it also produces a UnicodeDecodeError.   Actually,
    one of UTF-8's advantages is that there are invalid sequences of bytes,
    which helps to build robust systems:  mistakes in data won't be accepted as
    if they were valid.</p><figurep href="text/unipain/unipain.html#22"><img src="text/unipain_021.png" alt="Error handling"/></figurep><p>When encoding or decoding, you can specify what should happen when
    the codec can't handle the data.  An optional second argument to encode or
    decode specifies the policy.  The default value is "strict", which means
    raise an error, as we've seen.</p><p>A value of "replace" means, give me a standard replacement character.
    When encoding, the replacement character is a question mark, so any code
    point that can't be encoded using the specified encoding will simply
    produce a "?".</p><p>Other error handlers are more useful. "xmlcharrefreplace" produces
    an HTML/XML character entity reference, so that \u01B4 becomes "&amp;#436;"
    (hex 01B4 is decimal 436.)  This is very useful if you need to output
    unicode for an HTML file.</p><p>Notice that different error policies are used for different reasons.
    "Replace" is a defensive mechanism against data that cannot be interpreted,
    and loses information. "Xmlcharrefreplace" preserves all the original
    information, and is used when outputting data where XML escapes are
    acceptable.</p><figurep href="text/unipain/unipain.html#23"><img src="text/unipain_022.png" alt="Error handling"/></figurep><p>You can also specify error handling when decoding.  "Ignore" will drop
    bytes that can't decode properly.  "Replace" will insert a Unicode U+FFFD,
    "REPLACEMENT CHARACTER" for problem bytes.  Notice that since the decoder
    can't decode the data, it doesn't know how many Unicode characters were
    intended.  Decoding our UTF-8 bytes as ASCII produces 16 replacement
    characters, one for each byte that couldn't be decoded, while those bytes
    were meant to only produce 6 Unicode characters.</p><figurep href="text/unipain/unipain.html#24"><img src="text/unipain_023.png" alt="Implicit conversion"/></figurep><p>Python 2 tries to be helpful when working with unicode and byte strings.
    If you try to perform a string operation that combines a unicode string
    with a byte string, Python 2 will automatically decode the byte string to
    produce a second unicode string, then will complete the operation with the
    two unicode strings.</p><p>For example, we try to concatenate a unicode "Hello " with a byte string
    "world".  The result is a unicode "Hello world".  On our behalf, Python 2
    is decoding the byte string "world" using the ASCII codec.  The encoding
    used for these implicit decodings is the value of
    sys.getdefaultencoding().</p><p>The implicit encoding is ASCII because it's the only safe guess: ASCII
    is so widely accepted, and is a subset of so many encodings, that it's 
    unlikely to be wrong.</p><figurep href="text/unipain/unipain.html#25"><img src="text/unipain_024.png" alt="Implicit decoding errors"/></figurep><p>Of course, these implicit decodings are not immune to decoding errors.  If you
    try to combine a byte string with a unicode string and the byte string can't be
    decoded as ASCII, then the operation will raise a UnicodeDecodeError.</p><p>This is the source of those painful UnicodeErrors.  Your code
    inadvertently mixes unicode strings and byte strings, and as long as the
    data is all ASCII, the implicit conversions silently succeed.  Once a
    non-ASCII character finds its way into your program, an implicit decode
    will fail, causing a UnicodeDecodeError.</p><figurep href="text/unipain/unipain.html#26"><img src="text/unipain_025.png" alt="Python 2 is &#8220;helpful&#8221;"/></figurep><p>Python 2's philosophy was that unicode strings and byte strings are 
    confusing, and it tried to ease your burden by automatically converting
    between them, just as it does for ints and floats.  But the conversion
    from int to float can't fail, while byte string to unicode string can.</p><p>Python 2 silently glosses over byte to unicode conversions, making it
    much easier to write code that deals with ASCII.  The price you pay is
    that it will fail with non-ASCII data.</p><figurep href="text/unipain/unipain.html#27"><img src="text/unipain_026.png" alt="Other implicit conversions"/></figurep><p>There are lots of ways to combine two strings, and all of them will
    decode bytes to unicode, so you have to watch out for them.</p><p>Here we use an ASCII format string, with unicode data.  The format
    string will be decoded to unicode, then the formatting performed, resulting
    in a unicode string.</p><p>Next we switch the two:  A unicode format string and a byte string again
    combine to produce a unicode string, because the byte string data is
    decoded as ASCII.</p><p>Even just attempting to print a unicode string will cause an implicit
    encoding: output is always bytes, so the unicode string has to be encoded
    into bytes before it can be printed.</p><p>The next one is truly confusing: we ask to encode a byte string to UTF-8,
    and get an error about not being about to decode as ASCII!  The problem here
    is that byte strings can't be encoded: remember encode is how you turn unicode
    into bytes.  So to perform the encoding you want, Python 2 needs a unicode
    string, which it tries to get by implicitly decoding your bytes as ASCII.</p><p>So you asked to encode to UTF-8, and you get an error about decoding ASCII.
    It pays to look carefully at the error, it has clues about what operation is
    being attempted, and how it failed.</p><p>Lastly, we encode an ASCII string to UTF-8, which is silly, encode
    should be used on unicode string.  To make it work, Python performs the
    same implicit decode to get a unicode string we can encode, but since the
    string is ASCII, it succeeds, and then goes on to encode it as UTF-8,
    producing the original byte string, since ASCII is a subset of UTF-8.</p><figurep href="text/unipain/unipain.html#28"><img src="text/unipain_027.png" alt="Bytes and Unicode"/></figurep><p>This is the most important Fact of Life: bytes and unicode are both
    important, and you need to deal with both of them.  You can't pretend
    that everything is bytes, or everything is unicode.  You need to use 
    each for their purpose, and explicitly convert between them as needed.</p><h1>Python 3</h1><figurep href="text/unipain/unipain.html#29"><img src="text/unipain_028.png" alt="Python 3"/></figurep><p>We've seen the source of Unicode pain in Python 2, now let's take
    a look at Python 3.  The biggest change from Python 2 to Python 3 is 
    their treatment of Unicode.</p><figurep href="text/unipain/unipain.html#30"><img src="text/unipain_029.png" alt="Str vs bytes"/></figurep><p>Just as in Python 2, Python 3 has two string types, one for unicode and
    one for bytes, but they are named differently.</p><p>Now the "str" type that you get from a plain string literal stores unicode,
    and the "bytes" types stores bytes.  You can create a bytes literal with a b
    prefix.</p><p>So "str" in Python 2 is now called "bytes," and "unicode" in Python 2 is
    now called "str".  This makes more sense than the Python 2 names, since 
    Unicode is how you want all text stored, and byte strings are only for 
    when you are dealing with bytes.</p><figurep href="text/unipain/unipain.html#31"><img src="text/unipain_030.png" alt="No coercion!"/></figurep><p>The biggest change in the Unicode support in Python 3 is that there is
    no automatic decoding of byte strings.  If you try to combine a byte string
    with a unicode string, you will get an error all the time, regardless of
    the data involved!</p><p>All of those operations I showed where Python 2 silently converted byte
    strings to unicode strings to complete an operation, every one of them is
    an error in Python 3.</p><p>In addition, Python 2 considers a unicode string and a byte string
    equal if they contain the same ASCII bytes, and Python 3 won't.  A
    consequence of this is that unicode dictionary keys can't be found with
    byte strings, and vice-versa, as they can be in Python 2.</p><figurep href="text/unipain/unipain.html#32"><img src="text/unipain_031.png" alt="Python 3 pain"/></figurep><p>This drastically changes the nature of Unicode pain in Python 3.  In
    Python 2, mixing unicode and bytes succeeds so long as you only use ASCII
    data.  In Python 3, it fails immediately regardless of the data.</p><p>So Python 2's pain is deferred: you think your program is correct, and
    find out later that it fails with exotic characters.</p><p>With Python 3, your code fails immediately, so even if you are
    only handling ASCII, you have to explicitly deal with the difference
    between bytes and unicode.</p><p>Python 3 is strict about the difference between bytes and unicode.
    You are forced to be clear in your code which you are dealing with.
    This has been controversial, and can cause you pain.</p><figurep href="text/unipain/unipain.html#33"><img src="text/unipain_032.png" alt="Reading files"/></figurep><p>Because of this new strictness, Python 3 has changed how you read files.
    Python has always had two modes for reading files: binary and text.  In Python 2, it 
    only affected the line endings, and on Unix platforms, even that was a no-op.</p><p>In Python 3, the two modes produce different results.  When you open a file in text
    mode, either with "r", or by defaulting the mode entirely, the data read from the 
    file is implicitly decoded into Unicode, and you get str objects.</p><p>If you open a file in binary mode, by supplying "rb" as the mode, then the data
    read from the file is bytes, with no processing done on them.</p><p>The implicit conversion from bytes to unicode uses the encoding returned
    from locale.getpreferredencoding(), and it may not give you the results you
    expect.  For example, when we read hi_utf8.txt, it's being decoded using
    the locale's preferred encoding, which since I created these samples on
    Windows, is "cp1252".  Like ISO 8859-1, CP-1252 is a one-byte character
    code that will accept any byte value, so it will never raise a
    UnicodeDecodeError.  That also means that it will happily decode data that
    isn't actually CP-1252, and produce garbage.</p><p>To get the file read properly, you should specify an encoding to use.
    The open() function now has an optional encoding parameter.</p><h1>Pain relief</h1><figurep href="text/unipain/unipain.html#34"><img src="text/unipain_033.png" alt="Pain relief"/></figurep><p>OK, so how do we deal with all this pain? The good news it that the rules
    to remember are simple, and they're the same for Python 2 and Python 3.</p><figurep href="text/unipain/unipain.html#35"><img src="text/unipain_034.png" alt="Pro tip #1: Unicode sandwich"/></figurep><p>As we saw with Fact of Life #1, the data coming into and going out of
    your program must be bytes.  But you don't need to deal with bytes on the
    inside of your program.  The best strategy is to decode incoming bytes as
    soon as possible, producing unicode.  You use unicode throughout your
    program, and then when outputting data, encode it to bytes as late as
    possible.</p><p>This creates a Unicode sandwich: bytes on the outside, Unicode on the
    inside.</p><p>Keep in mind that sometimes, a library you're using may do some of these
    conversions for you. The library may present you with Unicode input, or
    will accept Unicode for output, and the library will take care of the edge
    conversion to and from bytes.  For example, Django provides Unicode, as
    does the json module.</p><figurep href="text/unipain/unipain.html#36"><img src="text/unipain_035.png" alt="Pro tip #2: Know what you have"/></figurep><p>The second rule is, you have to know what kind of data you are dealing with.
    At any point in your program, you need to know whether you have a byte string
    or a unicode string.  This shouldn't be a matter of guessing, it should be
    by design.</p><p>In addition, if you have a byte string, you should know what encoding it
    is if you ever intend to deal with it as text.</p><p>When debugging your code, you can't simply print a value to see what it
    is.  You need to look at the type, and you may need to look at the repr of
    the value in order to get to the bottom of what data you have.</p><figurep href="text/unipain/unipain.html#37"><img src="text/unipain_036.png" alt="Encoding is out-of-band"/></figurep><p>I said you have to understand what encoding your byte strings are.
    Here's Fact of Life #4: You can't determine the encoding of a byte string
    by examining it.  You need to know through other means. For example,
    many protocols include ways to specify the encoding.  Here we have
    examples from HTTP, HTML, XML, and Python source files.  You may
    also know the encoding by prior arrangement, for example, the spec
    for a data source may specify the encoding.</p><p>There are ways to guess at the encoding of the bytes, but they are
    just guesses. The only way to be sure of the encoding is to find it 
    out some other way.</p><figurep href="text/unipain/unipain.html#38"><img src="text/unipain_037.png" alt="Poo happens"/></figurep><p>Here's an example of our exotic Unicode string, encoded as UTF-8, and
    then mistakenly decoded in a variety of encodings.  As you can see, decoding
    with an incorrect encoding might succeed, but produce the wrong characters.
    Your program can't tell it's decoding wrong, only when people try to read
    the text will you know something has gone wrong.</p><p>This is a good demonstration of Fact of Life #4: the same stream of bytes
    is decodable using a number of different encodings.  The bytes themselves
    don't indicate what encoding they use.</p><p>BTW, there's a term for this garbage display, from the Japanese who have
    been dealing with this for years and years: Mojibake.</p><figurep href="text/unipain/unipain.html#39"><img src="text/unipain_038.png" alt="Data is dirty"/></figurep><p>Unfortunately, because the encoding for bytes has to be communicated
    separately from the bytes themselves, sometimes the specified encoding is
    wrong.  For example, you may pull an HTML page from a web server, and the
    HTTP header claims the page is 8859-1, but in fact, it is encoded with
    UTF-8.</p><p>In some cases, the encoding mismatch will succeed and cause mojibake.
    Other times, the encoding is invalid for the bytes, and will cause a
    UnicodeError of some sort.</p><figurep href="text/unipain/unipain.html#40"><img src="text/unipain_039.png" alt="Pro tip #3: Test Unicode"/></figurep><p>It should go without saying: you should explicitly test your Unicode
    support.  To do this, you need challenging Unicode data to pump through
    your code.  If you are an English-only speaker, you may have a problem
    doing this, because lots of non-ASCII data is hard to read.  Luckily, the
    variety of Unicode code points mean you can construct complex Unicode
    strings that are still readable by English speakers.</p><p>Here's an example of overly-accented text, readable pseudo-ASCII
    text, and upside-down text.  One good source of these sorts of strings
    are various web sites that offer strings like this for teenagers to
    paste into social networking sites.</p><figurep href="text/unipain/unipain.html#41"><img src="text/unipain_040.png" alt="More Unicode"/></figurep><p>Depending on your application, you may need to dig deeper into
    the other complexities in the Unicode world.  There are many
    details that I haven't covered here, and they can be very involved.
    I call this Fact of Life #5&#189; because you may not have to
    deal with any of this.</p><figurep href="text/unipain/unipain.html#42"><img src="text/unipain_041.png" alt="Facts of Life"/></figurep><p>To review, these are the five unavoidable Facts of Life:</p><ol><li>All input and output of your program is bytes.</li>
    <li>The world needs more than 256 symbols to communicate text.</li>
    <li>Your program has to deal with both bytes and Unicode.</li>
    <li>A stream of bytes can't tell you its encoding.</li>
    <li>Encoding specifications can be wrong.</li>
    </ol><figurep href="text/unipain/unipain.html#43"><img src="text/unipain_042.png" alt="Pro tips"/></figurep><p>These are the three Pro Tips to keep in mind as you build your software
    to keep your code Unicode-clean:</p><ol><li>Unicode sandwich: keep all text in your program as Unicode, and convert
    as close to the edges as possible.</li>

    <li>Know what your strings are: you should be able to explain which of your
    strings are Unicode, which are bytes, and for your byte strings, what 
    encoding they use.</li>

    <li>Test your Unicode support.  Use exotic strings throughout your test
    suites to be sure you're covering all the cases.</li>

    </ol><p>If you follow these tips, you'll write good solid code that deals
    well with Unicode, and won't fall over no matter how wild the Unicode
    it encounters.</p><figurep href="text/unipain/unipain.html#44"><img src="text/unipain_043.png" alt="See also"/></figurep><p>Other resources you might find helpful:</p><p>Joel Spolsky wrote <a href="http://www.joelonsoftware.com/articles/Unicode.html">The Absolute Minimum Every Software Developer Absolutely, Positively Must Know About Unicode and Character Sets (No Excuses!)</a>,
    which covers how Unicode works and why.  It has no Python-specific information,
    but is better written than this talk!</p><p>If you need to deal with the semantics of arbitrary Unicode characters, the <a href="http://docs.python.org/library/unicodedata.html">unicodedata module</a>
    in the Python standard library has functions that can help.</p><p>For testing Unicode, the various <a href="http://fsymbols.com/generators/encool">"fancy" text generators</a>
    for use on social networks work great.</p><figurep href="text/unipain/unipain.html#45"><img src="text/unipain_044.png" alt="Thank You"/></figurep>


    
    
    



    

    





    



    



    

    



    



    

    



    

    



</page>